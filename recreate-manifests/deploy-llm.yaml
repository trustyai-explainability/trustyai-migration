apiVersion: v1
kind: Service
metadata:
  name: minio-llms
spec:
  ports:
    - name: minio-client-port
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio-llms
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-models-claim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  # storageClassName: gp3-csi
  resources:
    requests:
      storage: 300Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-minio-container # <--- change this
labels:
    app: minio-llms # <--- change this to match label on the pod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio-llms  # <--- change this to match label on the pod
  template: # => from here down copy and paste the pods metadata: and spec: sections
    metadata:
      labels:
        app: minio-llms
        maistra.io/expose-route: 'true'
      name: minio-llms
    spec:
      volumes:
      - name: model-volume
        persistentVolumeClaim:
          claimName: vllm-models-claim
      initContainers:
        - name: download-model
          image: quay.io/rgeada/llm_downloader:latest
          securityContext:
            fsGroup: 1001
          command:
            - bash
            - -c
            - |
              models=(
                "Qwen/Qwen2.5-0.5B-Instruct" 
                #"microsoft/Phi-3-mini-4k-instruct"
              )
              echo "Starting download"
              for model in "${models[@]}"; do
                echo "Downloading $model"
                /tmp/venv/bin/huggingface-cli download $model --local-dir /mnt/models/llms/$(basename $model)
              done
              echo "Done!"
          resources:
            limits:
              memory: "2Gi"
              cpu: "2"
          volumeMounts:
            - mountPath: "/mnt/models/"
              name: model-volume
      containers:
        - args:
            - server
            - /models
          env:
            - name: MINIO_ACCESS_KEY
              value:  THEACCESSKEY
            - name: MINIO_SECRET_KEY
              value: THESECRETKEY
          image: quay.io/trustyai/modelmesh-minio-examples:latest
          name: minio
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: "/models/"
              name: model-volume
---
apiVersion: v1
kind: Secret
metadata:
  name: aws-connection-llm-data-connection
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/managed: 'true'
  annotations:
    opendatahub.io/connection-type: s3
    openshift.io/display-name: Minio Data Connection
data:
  AWS_ACCESS_KEY_ID: VEhFQUNDRVNTS0VZ
  AWS_DEFAULT_REGION: dXMtc291dGg=
  AWS_S3_BUCKET: bGxtcw==
  AWS_S3_ENDPOINT: aHR0cDovL21pbmlvLWxsbXM6OTAwMA==
  AWS_SECRET_ACCESS_KEY: VEhFU0VDUkVUS0VZ
type: Opaque
---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime
  annotations:
    openshift.io/display-name: vLLM ServingRuntime for KServe
    opendatahub.io/template-display-name: vLLM ServingRuntime for KServe
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: '8080'
    openshift.io/display-name: vLLM ServingRuntime for KServe
  labels:
    opendatahub.io/dashboard: 'true'
  containers:
    - args:
        - '--port=8080'
        - '--model=/mnt/models'
        - '--served-model-name={{.Name}}'
        - '--dtype=float16'
        - '--enforce-eager'
      command:
        - python
        - '-m'
        - vllm.entrypoints.openai.api_server
      env:
        - name: HF_HOME
          value: /tmp/hf_home
      image: 'quay.io/opendatahub/vllm:stable-849f0f5'
      # image: 'quay.io/rh-aiservices-bu/vllm-cpu-openai-ubi9@sha256:d680ff8becb6bbaf83dfee7b2d9b8a2beb130db7fd5aa7f9a6d8286a58cebbfd'
      name: kserve-container
      ports:
        - containerPort: 8080
          protocol: TCP
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llm
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    openshift.io/display-name: llm
    security.opendatahub.io/enable-auth: 'true'
    serving.knative.openshift.io/enablePassthrough: 'true'
    serving.kserve.io/deploymentMode: RawDeployment
    sidecar.istio.io/inject: 'true'
    sidecar.istio.io/rewriteAppHTTPProbers: 'true'
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '1'
          memory: 8Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '1'
          memory: 8Gi
          nvidia.com/gpu: '1'
      runtime: vllm-runtime
      storage:
        key: aws-connection-llm-data-connection
        path: Qwen2.5-0.5B-Instruct
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: llm-route
spec:
  tls:
    termination: reencrypt
    insecureEdgeTerminationPolicy: Redirect
  to:
    kind: Service
    name: llm-predictor
  port:
    targetPort: https
